## Variable aléatoire : introduction

## Introduction

Soit $\Omega$ l'ensemble fondamental des résultats d'une expérience aléatoire. L'attribution d'un nombre à chaque résultat de l'expérience permet de définir une variable aléatoire.

## Illustration

![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-35.jpg?height=341&width=480&top_left_y=388&top_left_x=80)

- À chaque événement élémentaire $\omega$ correspond un nombre réel $x$.
- $x=$ réalisation de la variable $X$ pour l'événement $\omega$
- Attention, il n'y a pas forcément autant de valeurs possibles $x$ que d'événements élémentaires (ex. 3)

[^0]
## Variable aléatoire : exemples

## Exemple 1 : lancé d'un dé à 6 faces

- Expérience aléatoire : lancé d'un dé à 6 faces
- Univers des événements élémentaires possibles : $\{1 ; 2 ; 3 ; 4 ; 5 ; 6\}$
- Valeurs possibles pour la va : $\{1 ; 2 ; 3 ; 4 ; 5 ; 6\}$


## Exemple 2 : facteur Rhésus

- Expérience aléatoire : détermination du facteur rhésus
- Univers des possibles : \{positif; négatif\}
- Valeurs possibles pour la va : $\{0 ; 1\}$ (codage arbitraire)


## Exemple 3 : nombre de filles dans une fratrie de 2

- Expérience aléatoire : constitution d'une fratrie de 2 enfants
- Univers des possibles : $\Omega=\{G G ; G F ; F G ; F F\}$
- Variable aléatoire $X$ : "Nombre de filles"
- Valeurs possibles pour la va $X: X(\Omega)=\{0 ; 1 ; 2\}$


## Variable aléatoire : définition

## Définition (à titre informatif)

On appelle variable aléatoire sur $\Omega$ tout application $X: \Omega \rightarrow \mathbb{R}$ telle que $\forall(a, b) \in \mathbb{R} \quad X^{-1}([a, b])$ est un évènement

## Notations

Les va sont notées avec des lettres majuscules : $X, Y, Z, \ldots$
Les valeurs possibles ou réalisations d'une va sont notées avec des
lettres minuscules: $x_{i}, a, z, \ldots$
Evènements : $(X=k),(0 \leq Z \leq 1), \ldots$

## Propriétés

Si $X$ et $Y$ sont des va définies sur $\Omega$ alors:

- $X+Y$ est une va
- $X \times Y$ est une va
- $\forall \lambda \in \mathbb{R}, \lambda X$ est une va


## Deux types de variables aléatoires

Variable aléatoire discrète
Elle prend un nombre fini ou infini dénombrable de valeurs possibles Exemples :

- Résultat d'un lancé de dé
- Nombre d'opérations réalisées dans un service de chirurgie

Variable aléatoire continue
Elle prend un nombre infini indénombrable de valeurs possibles Exemples :

- Taux de glucose sanguin
- Poids des nouveaux nés


## Loi de probabilité (1) : cas des va discrètes

## Définition

Soit $X$ une va discrète. Sa loi de probabilité est déterminée par

- I'ensemble des valeurs possibles $x_{i}(i \in I$, fini ou infini dénombrable)
- les probabilités $p_{i}=P\left(X=x_{i}\right)$

Propriétés

- $\forall i \in I, P\left(X=x_{i}\right) \geq 0$
- $\sum_{i \in I} P\left(X=x_{i}\right)=\sum_{i \in I} p_{i}=1$

Représentation classique

| valeur possible | $x_{1}$ | $\ldots$ | $x_{i}$ | $\ldots$ | $x_{n}$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| probabilité | $p_{1}$ | $\ldots$ | $p_{i}$ | $\ldots$ | $p_{n}$ |

## Exemple de loi de probabilité discrète

## Exemple de la fratrie de 2 enfants

- Hypothèse : Probabilité d'avoir un garçon $=0,5$

Distribution de probabilité ou loi de probabilité du nombre de filles dans la fratrie:

| événements possibles | GG | GF ou FG | FF |
| :---: | :---: | :---: | :---: |
| valeurs possibles | 0 | 1 | 2 |
| probabilités | $1 / 4$ | $1 / 2$ | $1 / 4$ |

- $P(X=0)=P(G \cap G)=P(G) \times P(G)$ (indépendance)
- $P(X=2)=P(F \cap F)=P(F) \times P(F)$ (indépendance)
- $P(X=1)=P((G \cap F) \cup(F \cap G))=P(G \cap F)+P(F \cap G)$ (incompatibilité)
$P(X=1)=1 / 4+1 / 4=1 / 2$


## Fonction de répartition : cas discret (1)

## Définition

On appelle fonction de répartition (fdr) de $X$ toute fonction $F$ telle que $\forall t \in \mathbb{R}, F(t)=P(X \leq t)$

## Interprétation

La fonction de répartition correspond à la distribution des probabilités cumulées

## Fonction de répartition : cas discret (2)

Exemple de la fratrie de 2 enfants

| Valeurs possibles | Probabilités |
| :---: | :---: |
| 0 | 0,25 |
| 1 | 0,5 |
| 2 | 0,25 |

$$
\begin{array}{cl}
t<0 & P(X \leq t)=0 \\
t=0 & P(X \leq t)=0,25 \\
0<t<1 & P(X \leq t)=0,25 \\
t=1 & P(X \leq t)=0,75 \\
1<t<2 & P(X \leq t)=0,75 \\
t=2 & P(X \leq t)=1 \\
t>2 & P(X \leq t)=1
\end{array}
$$

![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-42.jpg?height=331&width=510&top_left_y=578&top_left_x=80)
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-42.jpg?height=332&width=516&top_left_y=578&top_left_x=646)

## Fonction de répartition : cas discret (3)

Propriétés

- $\forall t \in \mathbb{R}, 0 \leq F(t) \leq 1$
- F est croissante
- $\lim _{x \rightarrow-\infty} F(x)=0$
- $\lim _{x \rightarrow+\infty} F(x)=1$

Dans le cas discret, F est une fonction «en marches d'escalier»
Calcul de probabilités

$$
\begin{gathered}
P\left(X=x_{i}\right)=P\left(X \leq x_{i}\right)-P\left(X \leq x_{i-1}\right) \\
P\left(X=x_{i}\right)=F\left(x_{i}\right)-F\left(x_{i-1}\right)
\end{gathered}
$$

## Loi de probabilité : cas des va continues

## Problème

L'ensemble des valeurs possibles de $X$ est infini indénombrable

- on ne peut définir la loi de probabilité par l'ensemble des $\left(x_{i}, p_{i}\right)$
- En général, $\forall i, P\left(X=x_{i}\right)=p_{i}=0$


## Densité de probabilité

On appelle densité de probabilité (ddp) toute fonction $f$ telle que :

- $\forall x \in \mathbb{R}, f(x) \geq 0$
- $\int_{-\infty}^{+\infty} f(x) d x=1$ (aire sous la courbe égale à 1 )


## Probabilité d'un intervalle

$$
P(a \leq X \leq b)=\int_{a}^{b} f(x) d x
$$

![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-44.jpg?height=273&width=418&top_left_y=663&top_left_x=724)

## Fonction de répartition : cas continu

Même définition que dans le cas discret
On appelle fonction de répartition (fdr) de $X$ toute fonction $F$ telle que $\forall x \in \mathbb{R}, F(x)=P(X \leq x)$

Mêmes propriétés que dans le cas discret (cf exercices)

- F est croissante
- $\lim _{x \rightarrow-\infty} F(x)=0$
- $\lim _{x \rightarrow+\infty} F(x)=1$

Cas continu: la fdr est continue et non plus en marches d'escalier
Lien avec la densité de probabilité - calcul de probabilités
Soit une va $X$ dont la ddp est $f . F: x \rightarrow F(x)=\int_{-\infty}^{x} f(t) d t$

$$
P(a \leq X \leq b)=\int_{a}^{b} f(t) d t=F(b)-F(a)
$$

Espérance d'une va
Introduction
Espérance = moyenne théorique
Elle renseigne sur la position des valeurs possibles sur une échelle
Définition dans le cas d'une va discrète $X$
L'espérance de $X$ se note $\mathrm{E}(X)$ ou $\mu_{X}$ et est définie par :

$$
\mathrm{E}(X)=\sum_{i} x_{i} \times P\left(X=x_{i}\right)=\sum_{i} x_{i} \times p_{i}
$$

Si $X(\Omega)$ est fini alors $i \in[0 ; n]$
Si $X(\Omega)$ est infini dénombrable alors $i \in[0 ;+\infty[$
Définition dans le cas d'une va continue
Soit $X$ une va continue et soit $f$ sa ddp. L'espérance de X est définie par

$$
\mathrm{E}(X)=\int_{-\infty}^{+\infty} x f(x) d x
$$

## Propriété de l'espérance

Linéarité
Soient $X$ et $Y$ deux va et a et $b$ des nombres réels :

$$
\begin{gathered}
\mathrm{E}(a X+b)=a \mathrm{E}(X)+b \\
\mathrm{E}(X+Y)=\mathrm{E}(X)+\mathrm{E}(Y)
\end{gathered}
$$

Variable centrée

- Si $\mathrm{E}(X)=0$ alors $X$ est une va centrée
- la va $Y=X-\mathrm{E}(X)$ est la va centrée associée à $X$


## Variance d'une va

Rappel : variance d'une distribution
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-48.jpg?height=257&width=498&top_left_y=173&top_left_x=115)

La variance d'une distribution mesure sa dispersion autour de sa moyenne

Définition générale (cas discret ou continu)
Soit $X$ une va. La variance de $X$ se note $\operatorname{var}(X)$ ou $\sigma_{X}^{2}$ et est définie par :

$$
\operatorname{var}(X)=\mathrm{E}\left((X-\mathrm{E}(X))^{2}\right)
$$

Autre formule équivalente, plus pratique pour les calculs

$$
\operatorname{var}(X)=\mathrm{E}\left(X^{2}\right)-\mathrm{E}(X)^{2}
$$

## Variance d'une va (2)

## Cas discret

Soit $X$ une va:

$$
\operatorname{var}(X)=\sum_{i}\left(x_{i}-\mathrm{E}(X)\right)^{2} \times P\left(X=x_{i}\right)
$$

Ou alors:
$\operatorname{var}(X)=\mathrm{E}\left(X^{2}\right)-\mathrm{E}(X)^{2} \quad$ avec $\mathrm{E}\left(X^{2}\right)=\sum_{i} x_{i}^{2} \times P\left(X=x_{i}\right)$

## Cas continu

Soit $f$ une ddp de la va $X$ :

$$
\operatorname{var}(X)=\int_{-\infty}^{+\infty}(x-E(X))^{2} \times f(x) d x
$$

Ou alors:

$$
\operatorname{var}(X)=\mathrm{E}\left(X^{2}\right)-\mathrm{E}(X)^{2} \quad \text { avec } \mathrm{E}\left(X^{2}\right)=\int_{-\infty}^{+\infty} x^{2} \times f(x) d x
$$

## Propriétés d'une variance

## Positivité

Une variance est toujours positive (ou nulle)
La variance n'est pas linéaire!
Soient $X$ et $Y$ deux va et $a$ et $b$ des nombres réels :

$$
\begin{gathered}
\operatorname{var}(a X)=a^{2} \operatorname{var}(X) \\
\operatorname{var}(X+b)=\operatorname{var}(X) \\
\operatorname{var}(a X+b)=a^{2} \operatorname{var}(X) \\
\operatorname{var}(X+Y)=\operatorname{var}(X)+\operatorname{var}(Y)+2 \operatorname{cov}(X, Y)
\end{gathered}
$$

Variable réduite

- Si $\operatorname{var}(X)=1$ alors $X$ est une va réduite


## Écart-type d'une va

## Définition

Soit $X$ une va. L'écart-type de $X$ se note $\sigma_{X}$ et se définit par :

$$
\sigma_{X}=\sqrt{\operatorname{var}(X)}
$$

Intérêt : mesure de dispersion dans la même unité de mesure que $X$

## Variable centrée réduite

Soit $X$ une va d'espérance $\mathrm{E}(X)$ et d'écart-type $\sigma_{X}$

$$
Z=\frac{X-E(X)}{\sigma_{X}}
$$

$Z$ est la va centrée réduite associée à $X$

- $\mathrm{E}(Z)=\mathrm{E}\left(\frac{X-E(X)}{\sigma_{X}}\right)=\frac{1}{\sigma_{X}} \times(\mathrm{E}(X)-\mathrm{E}(X))=0$
- $\operatorname{var}(Z)=\sigma_{Z}^{2}=\frac{1}{\sigma_{X}^{2}} \times \operatorname{var}(X-\mathrm{E}(X))=1$


## Variables aléatoires indépendantes

Rappel : évènements indépendants
Deux évènements A et B sont indépendants ssi $P(A \cap B)=P(A) P(B)$

## Définition dans le cas discret

Soient $X$ et $Y$ deux va indépendantes à valeurs respectivement dans $E=\left\{x_{1} ; x_{2} ; \ldots\right\}$ et $F=\left\{y_{1} ; y_{2} ; \ldots\right\}$

$$
P\left(X=x_{i}, Y=y_{j}\right)=P\left(X=x_{i}\right) \times P\left(Y=y_{j}\right) \quad \forall\left(x_{i}, y_{j}\right) \in E \times F
$$

## Propriétés

Si $X$ et $Y$ sont indépendantes, alors

- $\operatorname{cov}(X, Y)=0$ (Attention, réciproque fausse!)
- $\operatorname{var}(X+Y)=\operatorname{var}(X)+\operatorname{var}(Y)$

Si $X_{1}, X_{2}, \ldots, X_{n}$ sont indépendantes, alors

$$
\operatorname{var}\left(X_{1}+X_{2}+\ldots+X_{n}\right)=\operatorname{var}\left(X_{1}\right)+\operatorname{var}\left(X_{2}\right)+\ldots+\operatorname{var}\left(X_{n}\right)
$$

## Plan du cours

## Probabilités

## Probabilité conditionnelle

## Indépendance

## Variables aléatoires

Lois classiques
Lois discrètes : Bernoulli et binomiale
Lois continues : loi normale

## Loi de Bernoulli

## Loi de probabilité

Loi d'une va qui ne prend que 2 valeurs : 0 et 1

| $x_{i}$ | 0 | 1 |
| :---: | :---: | :---: |
| $p_{i}$ | q | p |

$$
X \rightarrow \operatorname{Bern}(p)
$$

![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-54.jpg?height=216&width=272&top_left_y=202&top_left_x=747)

Espérance et variance

$$
\begin{aligned}
& \mathrm{E}(X)=0 \times q+1 \times p=p \\
& \mathrm{E}\left(X^{2}\right)=0^{2} \times q+1^{2} \times p=p \\
& \operatorname{var}(X)=p-p^{2}=p(1-p)=p q
\end{aligned}
$$

$$
\begin{aligned}
\mathrm{E}(X) & =p \\
\operatorname{var}(X) & =p q
\end{aligned}
$$

## Utilisation

Modéliser les résultats d'expériences aléatoires ayant 2 issues possibles (épreuves de Bernoulli)
Exemple : statut maladie d'un individu

## Schéma de Bernoulli

## Principe

Répéter une épreuve de Bernoulli

- $n$ fois
- de façon indépendantes


## Exemple

Observer la présence (ou l'absence) d'effets indésirables sur un échantillon de 10 patients

## Modélisation

Ensemble de $n$ va indépendantes qui suivent toutes la même loi de Bernoulli de paramètre $p$

## Loi Binomiale (1) : introduction

Le problème
Dans la population, $85 \%$ des individus sont de rhésus positif. On considère un groupe de 5 patients : quelle est la probabilité que 2 d'entre eux soient de rhésus positif?

## Modélisation du problème

Pour un patient : soit $X$ la va représentant son groupe rhésus. On pose :

$$
\left\{\begin{array}{c}
X=1 \text { si l'individu est de } \mathrm{Rh}+ \\
X=0 \text { si l'individu est de } \mathrm{Rh}-
\end{array}\right.
$$

$X \rightarrow \operatorname{Bern}(p)$ avec $p=P(R h+)=0,85$
Pour les 5 patients : soit $S_{n}$ la va représentant le nombre de patients $\mathrm{Rh}+$ On répète 5 fois l'épreuve de Bernoulli de façon indépendante.

$$
S_{n}=X_{1}+X_{2}+X_{3}+X_{4}+X_{5} \quad S n \rightarrow \text { Binomiale }
$$

## Loi Binomiale (2) : loi de probabilité

Loi Binomiale et ses deux paramètres $n$ et $p$
Loi $\mathcal{B}(n, p)$ modélise la répétition de $n$ épreuves de Bernoulli indépendantes, de même probabilité $p$
$P\left(S_{n}=k\right)$ : probabilité de $k$ succès parmi les $n$ répétitions

$$
P\left(S_{n}=k\right)=C_{n}^{k}(p)^{k}(1-p)^{n-k} \quad \text { pour } k \in\{0,1,2, \ldots, n\}
$$

Rappels
$\mathrm{C}_{n}^{k}=\binom{n}{k}=\frac{n!}{k!(n-k)!} \quad \mathrm{Nb}$ combinaisons de $k$ éléments parmi $n$
$0!=1$
$n!=1 \times 2 \times \ldots \times n$

## Loi binomiale (3) : représentation graphique

Loi de probabilité
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-58.jpg?height=462&width=575&top_left_y=273&top_left_x=51)

## Loi binomiale (4)

Loi de probabilité et fonction de répartition
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-59.jpg?height=530&width=1102&top_left_y=245&top_left_x=85)

Loi binomiale (4) : espérance, variance et écart-type Soit $S_{n} \rightarrow \mathcal{B}(n, p)$
Formules

$$
\begin{gathered}
\mathrm{E}\left(S_{n}\right)=n p \\
\operatorname{var}\left(S_{n}\right)=n p q \\
\sigma_{S_{n}}=\sqrt{n p q}
\end{gathered}
$$

Comment retrouver les formules?

$$
\begin{aligned}
\mathrm{E}\left(S_{n}\right) & =\mathrm{E}\left(X_{1}+X_{2}+\ldots+X_{n}\right) \\
& =\mathrm{E}\left(X_{1}\right)+\mathrm{E}\left(X_{2}\right)+\ldots+\mathrm{E}\left(X_{n}\right) \\
& =p+p+\ldots+p=n p
\end{aligned}
$$

$$
\begin{aligned}
\operatorname{var}\left(S_{n}\right) & =\operatorname{var}\left(X_{1}+X_{2}+\ldots+X_{n}\right) \\
& =\operatorname{var}\left(X_{1}\right)+\operatorname{var}\left(X_{2}\right)+\ldots+\operatorname{var}\left(X_{n}\right) \quad \text { indépendance } \\
& =n p q
\end{aligned}
$$

## Loi normale ou loi de (Laplace-)Gauss

## Introduction

Loi la plus importante en statistiques

- Elle modélise de nombreux phénomènes
- Elle permet d'approximer plusieurs autres lois, en particulier quand on considère des grands échantillons
- Condition de nombreux tests statistiques

Paramètres
2 paramètres :

- $\mu$, son espérance
- $\sigma$, son écart-type (ou $\sigma^{2}$, sa variance)

Notation : $X \rightarrow \mathcal{N}(\mu ; \sigma)$ mais aussi $X \sim \mathcal{N}(\mu ; \sigma)$

## Loi normale (2) : densité de probabilité

Expression analytique
Soit une va $X$ suit une loi normale d'espérance $\mu$ et d'écart-type $\sigma$. Sa densité de probabilité est définie par :

$$
f(x)=\frac{1}{\sigma \times \sqrt{2 \pi}} \times \exp \left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right) \quad \forall x \in \mathbb{R}
$$

Représentation graphique
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-62.jpg?height=409&width=909&top_left_y=511&top_left_x=181)

## Propriétés

Représentation graphique
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-63.jpg?height=451&width=588&top_left_y=163&top_left_x=51)

- Symétrie par rapport à l'axe vertical passant par $\mu$
- 2 points d'inflexion d'abscisses $\mu-\sigma$ et $\mu+\sigma$
- Mode $=\mu=$ médiane
- ddp : aire sous la courbe $=1$

Combinaison linéaire de va Gaussiennes indépendantes
Soient $X_{1} \rightarrow \mathcal{N}\left(\mu_{1} ; \sigma_{1}\right)$ et $X_{2} \rightarrow \mathcal{N}\left(\mu_{2} ; \sigma_{2}\right)$ deux va Gaussiennes indépendantes

$$
Y=a X_{1}+b X_{2} \rightarrow \mathcal{N}\left(a \mu_{1}+b \mu_{2} ; \sqrt{a^{2} \sigma_{1}^{2}+b^{2} \sigma_{2}^{2}}\right)
$$

## Fonction de répartition

Expression analytique
Soit une va $X$ suit une loi normale d'espérance $\mu$ et d'écart-type $\sigma$. Sa fonction de répartition est donnée par :

$$
F(x)=P(X \leq x)=\frac{1}{\sigma \sqrt{2 \pi}} \int_{-\infty}^{x} \exp \left(-\frac{1}{2}\left(\frac{t-\mu}{\sigma}\right)^{2}\right) d t
$$

Calcul de probabilités

$$
P(a \leq X \leq b)=F(b)-F(a)
$$

L'intégrale ne peut se résoudre de façon algébrique $\rightarrow$ utilisation de tables

## La loi normale centrée-réduite (ou loi normale standard)

## Définition

Loi normale d'espérance $\mu=\mathbf{0}$ et d'écart-type $\sigma=\mathbf{1}$
Passage de $X \rightarrow \mathcal{N}(\mu ; \sigma)$ à $Z \rightarrow \mathcal{N}(0 ; 1)$ :

- On retranche $\mu$ (centrer)
- On divise par $\sigma$ (réduire)

$$
Z=\frac{X-\mu}{\sigma}
$$

Densité de probabilité

$$
f(z)=\frac{1}{\sqrt{2 \pi}} \times \exp \left(\frac{-1}{2} z^{2}\right)
$$

![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-65.jpg?height=438&width=571&top_left_y=466&top_left_x=591)

## Propriété de loi normale centrée-réduite

Représentation graphique
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-66.jpg?height=246&width=465&top_left_y=180&top_left_x=85)

- Symétrie par rapport à l'axe des ordonnées
- 2 points d'inflexion d'abscisses -1 et +1
- Mode $=0=$ médiane
- ddp : aire sous la courbe $=1$

Calcul de probabilités
On note $\Phi$ la fdr de la loi normale centrée réduite
$P(Z \leq z)=\Phi(z)$
$P(Z \geq z)=1-P(Z \leq z)=1-\Phi(z)$
$P(Z \geq z)=P(Z \leq-z)=\Phi(-z)$ (symétrie de la ddp/axe ordonnées)

$$
\Phi(-z)=1-\Phi(z)
$$

$$
P(a \leq Z \leq b)=P(a<Z<b)=\Phi(b)-\Phi(a)
$$

La table de la fonction de répartition (table 1)
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-67.jpg?height=246&width=856&top_left_y=131&top_left_x=208)

Pour une valeur de $z$ donné, la table donne $P(Z \leq z)$
Exemple
Lire dans la table

- $P(Z \leq 1,42)$
- $P(Z \leq-0,21)$
- $P(Z \geq 0,41)$


## La table de la fonction de répartition (table 1)

![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-68.jpg?height=247&width=855&top_left_y=128&top_left_x=208)

Pour une valeur de $z$ donné, la table donne $P(Z \leq z)$

## Exemple

Lire dans la table

- $P(Z \leq 1,42)$ - $P(Z \leq 1,42)=\Phi(1,42)=0,9222$
- $P(Z \leq-0,21) \quad P(Z \leq-0,21)=\Phi(-0,21)=1-\Phi(0,21) P(Z \leq-0,21)=1-0,5832=0,4168$
- $P(Z \geq 0,41) \quad-P(Z \geq 0,41)=1-P(Z \leq 0,41)=1-\Phi(0,41) P(Z \geq 0,41)=1-0,6591=0,3409$

Table de la fdr pour $p$ donné (table 2)
Attention, changement de table par rapport à 2023/2024
![](https://cdn.mathpix.com/cropped/2025_09_17_68f9d5d55601c91876fcg-69.jpg?height=247&width=856&top_left_y=167&top_left_x=208)

Pour une probabilité $p$ donnée, la table donne $z$ tel que $P(Z \leq z)=p$
Exemple
Lire dans la table la valeur de $z$ telle que :

- $P(Z \geq z)=0,025 \rightarrow P(Z \leq z)=1-0,025=0,975 \rightarrow z=1,96$
- $P(Z \geq z)=0,75 \quad-P(Z \geq z)=0,75 \rightarrow P(Z \leq-z)=0,75$
(Symétrie de la courbe)
on lit $-z=0,6745$, donc $z=-0,6745$


## Le théorème central limite (TCL)

## Énoncé simplifié

Soient $X_{1}, X_{2}, \ldots, X_{n}$ des variables mutuellement indépendantes de même loi de probabilité $\mathcal{L}(\mu, \sigma)$, alors lorsque $n$ est suffisamment grand la variable aléatoire $S_{n}=X_{1}+X_{2}+\ldots+X_{n}$ suit approximativement une loi normale d'espérance $n \times \mu$, et d'écart-type $\sqrt{n} \times \sigma$

Conséquence pour la variable $M$ (moyenne d'échantillon)
$M=\frac{X_{1}+X_{2}+\ldots+X_{n}}{n}$
Lorsque $n$ est suffisamment grand, alors $M$ suit approximativement une loi normale d'espérance $\mu_{M}=\mu$ et d'écart-type $\sigma_{m}=\frac{\sigma}{\sqrt{n}}$

Remarques

- Dans ce cours, on considèrera $n$ suffisamment grand quand $\mathbf{n} \geq \mathbf{3 0}$
- Le TCL explique l'importance de la loi normale dans la nature


## Approximation de la loi binomiale par la loi normale

## Approximation

Soit $S_{n} \rightarrow \mathcal{B}(n, p)$ et soit $X \rightarrow \operatorname{Bern}(p)$.

$$
S_{n}=\sum_{i=1}^{n} X_{i}, \text { les } X_{i} \text { étant indépendantes et de même loi que } X
$$

D'après le TCL, on peut approcher la loi de $S_{n}$ par une loi normale $\mathcal{N}(n p ; \sqrt{n p q})$.

En pratique
Si $n \geq 30, n p \geq 5$ et $n(1-p) \geq 5$, on approchera $\mathcal{B}(n, p)$ par $\mathcal{N}(n p ; \sqrt{n p q})$


[^0]:    Remarque
    Une va est une variable quantitative

